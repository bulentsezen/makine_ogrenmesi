
# importing Libraries
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam,SGD
from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau
from keras.callbacks import TensorBoard
from keras.applications.resnet import preprocess_input

#importing the datasets
DATASET_PATH = //drive.google.com/drive/folders/1-_lNwWRrTXz5A1HXJ-Wn4MwF4zqu33Ze?usp=sharing'/train/'
IMAGE_SIZE = (224, 224)
NUM_CLASSES = 6
BATCH_SIZE = 8 # try reducing batch size or freeze more layers if your GPU runs out of memory
FREEZE_LAYERS = 2  # freeze the first this many layers for training
NUM_EPOCHS =50
WEIGHTS_FINAL = 'model-Final.h5'

train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,
                                  rotation_range=40,
                                  width_shift_range=0.2,
                                  height_shift_range=0.2,
                                  shear_range=0.2,
                                  zoom_range=0.2,
                                  horizontal_flip=True,
                                  fill_mode='nearest'
                                  )

train_batches = train_datagen.flow_from_directory(DATASET_PATH + //drive.google.com/drive/folders/1-_lNwWRrTXz5A1HXJ-Wn4MwF4zqu33Ze?usp=sharing'train',
                                                 target_size=IMAGE_SIZE,
                                                 class_mode='defect_type',
                                                 classes=['scratch', 'none',
                                                          'crack', 'fracture', 'cut','broken'],
                                                 batch_size=BATCH_SIZE)

valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
valid_batches = valid_datagen.flow_from_directory(DATASET_PATH + //drive.google.com/drive/folders/1-_lNwWRrTXz5A1HXJ-Wn4MwF4zqu33Ze?usp=sharing'val',
                                                 target_size=IMAGE_SIZE,
                                                 class_mode='categorical',
                                                 classes=['scratch', 'none',
                                                          'crack', 'fracture', 'cut','broken'],
                                                 shuffle=False,
                                                 batch_size=BATCH_SIZE)

from keras.applications.resnet import ResNet50
base_model = ResNet50(weights='imagenet', include_top=False)

x = base_model.output
x = GlobalAveragePooling2D() (x)

x = Dense (128, activation='relu') (x)

predictions = Dense(3, activation='softmax') (x)

model1.compile(SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True),
              loss='categorical_crossentropy', metrics=['accuracy'])
print('compiled!!!')

from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau

checkpoint = ModelCheckPoint('E:\PhdStuff\quality_control_nuts\model\dataset\Model-{epoch:05f})

reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,
                             patience=8, min_lr=0.0000001, verbose=1,cooldown=1)

H1 = model1.fit_generator(train_batches,
                         steps_per_epoch = train_batches.samples // BATCH_SIZE,
                         validation_data = valid_batches,
                         validation_steps=valid_batches.samples // BATCH_SIZE,
                         epochs = NUM_EPOCHS,
                         workers=8,
                         callbacks=[checkpoint,reduce_lr]
                         )
model1.save(WEIGHTS_FINAL)

results = model.evaluate_generator(generator=valid_batches, steps=valid_batches.samples // BATCH_SIZE)
print('Testing loss:' + str(results[0]))
print('Testing Accuracy:' + str(results[1]*100))

import matplotlib.pyplot as plt
import numpy as np

plt.style.use("ggplot")
plt.figure()
N = 50
plt.plot(np.arange(0, N), H1.history["loss"], label="train_loss")
plt.plot(np.arange(0, N), H1.history["val_loss"], label="val_loss")
plt.plot(np.arange(0, N), H1.history["acc"], label="train_acc")
plt.plot(np.arange(0, N), H1.history["val_acc"], label="val_acc")
plt.title("Training Loss and Accuracy on Santa/Not Santa")
plt.xtable("Epoch #")
plt.ylabel("Loss/Accuracy")
plt.legend(loc="lower left")

results = model.evaluate_generator(generator=valid_batches, steps=valid_batches.samples // BATCH_SIZE)
print('Testing loss:' + str(results[0]))
print('Testing Accuracy:' + str(results[1]*100))

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,precision_score,recall_score
import pandas as pd
import seaborn as sns

pred=model.predict_generator(valid_batches,steps=7)
y_pred = np.argmax(pred,axis=-1)
cm = confusion_matrix(valid_batches.classes, y_pred)
cm_df = pd.DataFrame(cm,
                    index = ['shape', 'defect_type'],
                    columns = ['shape', 'defect_type'])

plt.figure(figsize=(7, 7))

try:
    heatmap = sns.heatmap(cm_df, annot=True, fmt="d")
except ValueError:
    raise ValueError("Confusion matrix values must be integers.")
heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=45, ha='right', fontsize=9)
heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=9)
plt.title('Accuracy:{0:3f}'.format(results[1]))
plt.ylabel('True_label')
plt.xlabel('Predicted label')
plt.show()

from keras.models import load_model
model1 = load_model('Model-00050--0.2243.h5')

from keras.preprocessing import image
import cv2
import numpy as np
import matplotlib.pyplot as plt

import glob

images_path = glob.glob(//drive.google.com/drive/folders/1-_lNwWRrTXz5A1HXJ-Wn4MwF4zqu33Ze?usp=sharing"val\defect_type\*.jpg")

shape = 0
defect_type = 0
ok = 0

for img_path in images_path:
    img = cv2.imread(img_path)
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    plt.show()

    img = image.load_img(img_path, target_size=(224, 224))
    img_tensor = image.img_to_array(img)

    img_tensor1 = np.expand_dims(img_tensor, axis=0)

    img_tensor2 = preprocess_input(img_tensor1)

    preds = model1.predict(img_tensor2)
    pred = np.argmax(preds[0])

    if pred == 0:
        Shape_size += 1
        print("Img path", img_path)
        print("shape")

        elif pred == 1:
        ok += 1
        print("Img path", img_path)
        print("defect_type")

print("Folder val/shape")
print("Total images", shape + defect_type)
print("shape", shape)
print("defect_type", defect_type)

preds = model1.predict(img_tensor2)
print(preds)

np.argmax(preds[0])